{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8456a63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "import quadprog\n",
    "\n",
    "\n",
    "# Auxiliary functions useful for GEM's inner optimization.\n",
    "\n",
    "def compute_offsets(task, nc_per_task, is_cifar):\n",
    "    \"\"\"\n",
    "        Compute offsets for cifar to determine which\n",
    "        outputs to select for a given task.\n",
    "    \"\"\"\n",
    "    if is_cifar:\n",
    "        offset1 = task * nc_per_task\n",
    "        offset2 = (task + 1) * nc_per_task\n",
    "    else:\n",
    "        offset1 = 0\n",
    "        offset2 = nc_per_task\n",
    "    return offset1, offset2\n",
    "\n",
    "\n",
    "def store_grad(pp, grads, grad_dims, tid):\n",
    "    \"\"\"\n",
    "        This stores parameter gradients of past tasks.\n",
    "        pp: parameters\n",
    "        grads: gradients\n",
    "        grad_dims: list with number of parameters per layers\n",
    "        tid: task id\n",
    "    \"\"\"\n",
    "    # store the gradients\n",
    "    grads[:, tid].fill_(0.0)\n",
    "    cnt = 0\n",
    "    for param in pp():\n",
    "        if param.grad is not None:\n",
    "            beg = 0 if cnt == 0 else sum(grad_dims[:cnt])\n",
    "            en = sum(grad_dims[:cnt + 1])\n",
    "            grads[beg: en, tid].copy_(param.grad.data.view(-1))\n",
    "        cnt += 1\n",
    "\n",
    "\n",
    "def overwrite_grad(pp, newgrad, grad_dims):\n",
    "    \"\"\"\n",
    "        This is used to overwrite the gradients with a new gradient\n",
    "        vector, whenever violations occur.\n",
    "        pp: parameters\n",
    "        newgrad: corrected gradient\n",
    "        grad_dims: list storing number of parameters at each layer\n",
    "    \"\"\"\n",
    "    cnt = 0\n",
    "    for param in pp():\n",
    "        if param.grad is not None:\n",
    "            beg = 0 if cnt == 0 else sum(grad_dims[:cnt])\n",
    "            en = sum(grad_dims[:cnt + 1])\n",
    "            this_grad = newgrad[beg: en].contiguous().view(\n",
    "                param.grad.data.size())\n",
    "            param.grad.data.copy_(this_grad)\n",
    "        cnt += 1\n",
    "\n",
    "\n",
    "def project2cone2(gradient, memories, margin=0.5, eps=1e-3):\n",
    "    \"\"\"\n",
    "        Solves the GEM dual QP described in the paper given a proposed\n",
    "        gradient \"gradient\", and a memory of task gradients \"memories\".\n",
    "        Overwrites \"gradient\" with the final projected update.\n",
    "        input:  gradient, p-vector\n",
    "        input:  memories, (t * p)-vector\n",
    "        output: x, p-vector\n",
    "    \"\"\"\n",
    "    memories_np = memories.cpu().t().double().numpy()\n",
    "    gradient_np = gradient.cpu().contiguous().view(-1).double().numpy()\n",
    "    t = memories_np.shape[0]\n",
    "    P = np.dot(memories_np, memories_np.transpose())\n",
    "    P = 0.5 * (P + P.transpose()) + np.eye(t) * eps\n",
    "    q = np.dot(memories_np, gradient_np) * -1\n",
    "    G = np.eye(t)\n",
    "    h = np.zeros(t) + margin\n",
    "    v = quadprog.solve_qp(P, q, G, h)[0]\n",
    "    x = np.dot(v, memories_np) + gradient_np\n",
    "    gradient.copy_(torch.Tensor(x).view(-1, 1))\n",
    "\n",
    "\n",
    "class GNet(nn.Module):\n",
    "    def __init__(self,\n",
    "                 n_inputs,\n",
    "                 n_outputs,\n",
    "                 n_tasks):\n",
    "        super(GNet, self).__init__()\n",
    "        nl, nh = 2, 100\n",
    "        self.margin = 0.5\n",
    "        self.is_cifar = False\n",
    "        if self.is_cifar:\n",
    "            self.net = ResNet18(n_outputs)\n",
    "        else:\n",
    "            self.net = MLP([n_inputs] + [nh] * nl + [n_outputs])\n",
    "\n",
    "        self.ce = nn.CrossEntropyLoss()\n",
    "        self.n_outputs = n_outputs\n",
    "\n",
    "        self.opt = optim.SGD(self.parameters(), 1e-3)\n",
    "\n",
    "        self.n_memories = 10\n",
    "        self.gpu = True\n",
    "\n",
    "        # allocate episodic memory\n",
    "        self.memory_data = torch.FloatTensor(\n",
    "            n_tasks, self.n_memories, n_inputs)\n",
    "        self.memory_labs = torch.LongTensor(n_tasks, 10)\n",
    "        if True:\n",
    "            self.memory_data = self.memory_data.cuda()\n",
    "            self.memory_labs = self.memory_labs.cuda()\n",
    "\n",
    "        # allocate temporary synaptic memory\n",
    "        self.grad_dims = []\n",
    "        for param in self.parameters():\n",
    "            self.grad_dims.append(param.data.numel())\n",
    "        self.grads = torch.Tensor(sum(self.grad_dims), n_tasks)\n",
    "        if True:\n",
    "            self.grads = self.grads.cuda()\n",
    "\n",
    "        # allocate counters\n",
    "        self.observed_tasks = []\n",
    "        self.old_task = -1\n",
    "        self.mem_cnt = 0\n",
    "        if self.is_cifar:\n",
    "            self.nc_per_task = int(n_outputs / n_tasks)\n",
    "        else:\n",
    "            self.nc_per_task = n_outputs\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        output = self.net(x)\n",
    "        if self.is_cifar:\n",
    "            # make sure we predict classes within the current task\n",
    "            offset1 = int(t * self.nc_per_task)\n",
    "            offset2 = int((t + 1) * self.nc_per_task)\n",
    "            if offset1 > 0:\n",
    "                output[:, :offset1].data.fill_(-10e10)\n",
    "            if offset2 < self.n_outputs:\n",
    "                output[:, offset2:self.n_outputs].data.fill_(-10e10)\n",
    "        return output\n",
    "\n",
    "    def observe(self, x, t, y):\n",
    "        # update memory\n",
    "        if t != self.old_task:\n",
    "            self.observed_tasks.append(t)\n",
    "            self.old_task = t\n",
    "\n",
    "        # Update ring buffer storing examples from current task\n",
    "        bsz = y.data.size(0)\n",
    "        endcnt = min(self.mem_cnt + bsz, self.n_memories)\n",
    "        effbsz = endcnt - self.mem_cnt\n",
    "        self.memory_data[t, self.mem_cnt: endcnt].copy_(\n",
    "            x.data[: effbsz])\n",
    "        if bsz == 1:\n",
    "            self.memory_labs[t, self.mem_cnt] = y.data[0]\n",
    "        else:\n",
    "            self.memory_labs[t, self.mem_cnt: endcnt].copy_(\n",
    "                y.data[: effbsz])\n",
    "        self.mem_cnt += effbsz\n",
    "        if self.mem_cnt == self.n_memories:\n",
    "            self.mem_cnt = 0\n",
    "\n",
    "        # compute gradient on previous tasks\n",
    "        if len(self.observed_tasks) > 1:\n",
    "            for tt in range(len(self.observed_tasks) - 1):\n",
    "                self.zero_grad()\n",
    "                # fwd/bwd on the examples in the memory\n",
    "                past_task = self.observed_tasks[tt]\n",
    "\n",
    "                offset1, offset2 = compute_offsets(past_task, self.nc_per_task,\n",
    "                                                   self.is_cifar)\n",
    "                ptloss = self.ce(\n",
    "                    self.forward(\n",
    "                        self.memory_data[past_task],\n",
    "                        past_task)[:, offset1: offset2],\n",
    "                    self.memory_labs[past_task] - offset1)\n",
    "                ptloss.backward()\n",
    "                store_grad(self.parameters, self.grads, self.grad_dims,\n",
    "                           past_task)\n",
    "\n",
    "        # now compute the grad on the current minibatch\n",
    "        self.zero_grad()\n",
    "\n",
    "        offset1, offset2 = compute_offsets(t, self.nc_per_task, self.is_cifar)\n",
    "        loss = self.ce(self.forward(x, t)[:, offset1: offset2], y - offset1)\n",
    "        loss.backward()\n",
    "\n",
    "        # check if gradient violates constraints\n",
    "        if len(self.observed_tasks) > 1:\n",
    "            # copy gradient\n",
    "            store_grad(self.parameters, self.grads, self.grad_dims, t)\n",
    "            indx = torch.cuda.LongTensor(self.observed_tasks[:-1]) if self.gpu \\\n",
    "                else torch.LongTensor(self.observed_tasks[:-1])\n",
    "            dotp = torch.mm(self.grads[:, t].unsqueeze(0),\n",
    "                            self.grads.index_select(1, indx))\n",
    "            if (dotp < 0).sum() != 0:\n",
    "                project2cone2(self.grads[:, t].unsqueeze(1),\n",
    "                              self.grads.index_select(1, indx), self.margin)\n",
    "                # copy gradients back\n",
    "                overwrite_grad(self.parameters, self.grads[:, t],\n",
    "                               self.grad_dims)\n",
    "        self.opt.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae75e45a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2ab8c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import importlib\n",
    "import datetime\n",
    "import argparse\n",
    "import random\n",
    "import uuid\n",
    "import time\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "# continuum iterator #########################################################\n",
    "\n",
    "\n",
    "def load_datasets():\n",
    "    d_tr, d_te = torch.load(\"/contai/avacl/notebooks/GradientEpisodicMemory/data/mnist_permutations.pt\")\n",
    "    n_inputs = d_tr[0][1].size(1)\n",
    "    n_outputs = 0\n",
    "    for i in range(len(d_tr)):\n",
    "        n_outputs = max(n_outputs, d_tr[i][2].max().item())\n",
    "        n_outputs = max(n_outputs, d_te[i][2].max().item())\n",
    "    return d_tr, d_te, n_inputs, n_outputs + 1, len(d_tr)\n",
    "\n",
    "\n",
    "class Continuum:\n",
    "\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.batch_size = 10\n",
    "        n_tasks = len(data)\n",
    "        task_permutation = range(n_tasks)\n",
    "\n",
    "        if True:\n",
    "            task_permutation = torch.randperm(n_tasks).tolist()\n",
    "\n",
    "        sample_permutations = []\n",
    "\n",
    "        for t in range(n_tasks):\n",
    "            N = data[t][1].size(0)\n",
    "            if 10 <= 0:\n",
    "                n = N\n",
    "            else:\n",
    "                n = min(10, N)\n",
    "\n",
    "            p = torch.randperm(N)[0:n]\n",
    "            sample_permutations.append(p)\n",
    "\n",
    "        self.permutation = []\n",
    "\n",
    "        for t in range(n_tasks):\n",
    "            task_t = task_permutation[t]\n",
    "            for _ in range(5):\n",
    "                task_p = [[task_t, i] for i in sample_permutations[task_t]]\n",
    "                random.shuffle(task_p)\n",
    "                self.permutation += task_p\n",
    "\n",
    "        self.length = len(self.permutation)\n",
    "        self.current = 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def next(self):\n",
    "        return self.__next__()\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.current >= self.length:\n",
    "            raise StopIteration\n",
    "        else:\n",
    "            ti = self.permutation[self.current][0]\n",
    "            j = []\n",
    "            i = 0\n",
    "            while (((self.current + i) < self.length) and\n",
    "                   (self.permutation[self.current + i][0] == ti) and\n",
    "                   (i < self.batch_size)):\n",
    "                j.append(self.permutation[self.current + i][1])\n",
    "                i += 1\n",
    "            self.current += i\n",
    "            j = torch.LongTensor(j)\n",
    "            return self.data[ti][1][j], ti, self.data[ti][2][j]\n",
    "\n",
    "# train handle ###############################################################\n",
    "\n",
    "\n",
    "def eval_tasks(model, tasks):\n",
    "    model.eval()\n",
    "    result = []\n",
    "    for i, task in enumerate(tasks):\n",
    "        t = i\n",
    "        x = task[1]\n",
    "        y = task[2]\n",
    "        rt = 0\n",
    "        \n",
    "        eval_bs = x.size(0)\n",
    "\n",
    "        for b_from in range(0, x.size(0), eval_bs):\n",
    "            b_to = min(b_from + eval_bs, x.size(0) - 1)\n",
    "            if b_from == b_to:\n",
    "                xb = x[b_from].view(1, -1)\n",
    "                yb = torch.LongTensor([y[b_to]]).view(1, -1)\n",
    "            else:\n",
    "                xb = x[b_from:b_to]\n",
    "                yb = y[b_from:b_to]\n",
    "            if True:\n",
    "                xb = xb.cuda()\n",
    "            _, pb = torch.max(model(xb, t).data.cpu(), 1, keepdim=False)\n",
    "            rt += (pb == yb).float().sum()\n",
    "\n",
    "        result.append(rt / x.size(0))\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def life_experience(model, continuum, x_te):\n",
    "    result_a = []\n",
    "    result_t = []\n",
    "\n",
    "    current_task = 0\n",
    "    time_start = time.time()\n",
    "\n",
    "    for (i, (x, t, y)) in enumerate(continuum):\n",
    "        if(((i % 100) == 0) or (t != current_task)):\n",
    "            result_a.append(eval_tasks(model, x_te))\n",
    "            result_t.append(current_task)\n",
    "            current_task = t\n",
    "\n",
    "        v_x = x.view(x.size(0), -1)\n",
    "        v_y = y.long()\n",
    "\n",
    "        if True:\n",
    "            v_x = v_x.cuda()\n",
    "            v_y = v_y.cuda()\n",
    "\n",
    "        model.train()\n",
    "        model.observe(v_x, t, v_y)\n",
    "\n",
    "    result_a.append(eval_tasks(model, x_te))\n",
    "    result_t.append(current_task)\n",
    "\n",
    "    time_end = time.time()\n",
    "    time_spent = time_end - time_start\n",
    "\n",
    "    return torch.Tensor(result_t), torch.Tensor(result_a), time_spent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63371214",
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(result_t, result_a, fname=None):\n",
    "    nt, changes = task_changes(result_t)\n",
    "\n",
    "    baseline = result_a[0]\n",
    "    changes = torch.LongTensor(changes + [result_a.size(0)]) - 1\n",
    "    result = result_a[changes]\n",
    "\n",
    "    # acc[t] equals result[t,t]\n",
    "    acc = result.diag()\n",
    "    fin = result[nt - 1]\n",
    "    # bwt[t] equals result[T,t] - acc[t]\n",
    "    bwt = result[nt - 1] - acc\n",
    "\n",
    "    # fwt[t] equals result[t-1,t] - baseline[t]\n",
    "    fwt = torch.zeros(nt)\n",
    "    for t in range(1, nt):\n",
    "        fwt[t] = result[t - 1, t] - baseline[t]\n",
    "\n",
    "    if fname is not None:\n",
    "        f = open(fname, 'w+')\n",
    "        print(' '.join(['%.4f' % r for r in baseline]), file=f)\n",
    "        print('|', file=f)\n",
    "        for row in range(result.size(0)):\n",
    "            print(' '.join(['%.4f' % r for r in result[row]]), file=f)\n",
    "        print('', file=f)\n",
    "        # print('Diagonal Accuracy: %.4f' % acc.mean(), file=f)\n",
    "        print('Final Accuracy: %.4f' % fin.mean(), file=f)\n",
    "        print('Backward: %.4f' % bwt.mean(), file=f)\n",
    "        print('Forward:  %.4f' % fwt.mean(), file=f)\n",
    "        f.close()\n",
    "\n",
    "    stats = []\n",
    "    # stats.append(acc.mean())\n",
    "    stats.append(fin.mean())\n",
    "    stats.append(bwt.mean())\n",
    "    stats.append(fwt.mean())\n",
    "\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f9394de3",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect. For debugging consider passing CUDA_LAUNCH_BLOCKING=1. CUDA error: device-side assert triggered",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-----------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-d108e28b464d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# initialize seeds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/random.py\u001b[0m in \u001b[0;36mmanual_seed\u001b[0;34m(seed)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_in_bad_fork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdefault_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/cuda/random.py\u001b[0m in \u001b[0;36mmanual_seed_all\u001b[0;34m(seed)\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0mdefault_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m     \u001b[0m_lazy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_call\u001b[0;34m(callable)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_lazy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;31m# TODO(torch_deploy): this accesses linecache, which attempts to read the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/cuda/random.py\u001b[0m in \u001b[0;36mcb\u001b[0;34m()\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0mdefault_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_generators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0mdefault_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0m_lazy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect. For debugging consider passing CUDA_LAUNCH_BLOCKING=1. CUDA error: device-side assert triggered"
     ]
    }
   ],
   "source": [
    "uid = uuid.uuid4().hex\n",
    "\n",
    "# initialize seeds\n",
    "torch.backends.cudnn.enabled = True\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "if True:\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "# load data\n",
    "x_tr, x_te, n_inputs, n_outputs, n_tasks = load_datasets()\n",
    "\n",
    "# set up continuum\n",
    "continuum = Continuum(x_tr)\n",
    "\n",
    "# load model\n",
    "#Model = importlib.import_module('model.' + 'gem')\n",
    "model = GNet(n_inputs, n_outputs, n_tasks)\n",
    "if True:\n",
    "    model.cuda()\n",
    "\n",
    "# run model on continuum\n",
    "result_t, result_a, spent_time = life_experience(\n",
    "    model, continuum, x_te)\n",
    "\n",
    "# prepare saving path and file name\n",
    "if not os.path.exists(\"results/\"):\n",
    "    os.makedirs(\"results/\")\n",
    "\n",
    "fname = 'Model' + '_' + 'mnist' + '_'\n",
    "fname += datetime.datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "fname += '_' + uid\n",
    "fname = os.path.join('results/', fname)\n",
    "\n",
    "# save confusion matrix and print one line of stats\n",
    "stats = confusion_matrix(result_t, result_a, fname + '.txt')\n",
    "\n",
    "one_liner = str('args') + ' # '\n",
    "one_liner += ' '.join([\"%.3f\" % stat for stat in stats])\n",
    "print(fname + ': ' + one_liner + ' # ' + str(spent_time))\n",
    "\n",
    "# save all results in binary file\n",
    "torch.save((result_t, result_a, model.state_dict(),\n",
    "            stats, one_liner), fname + '.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b7d33ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def task_changes(result_t):\n",
    "    n_tasks = int(result_t.max() + 1)\n",
    "    changes = []\n",
    "    current = result_t[0]\n",
    "    for i, t in enumerate(result_t):\n",
    "        if t != current:\n",
    "            changes.append(i)\n",
    "            current = t\n",
    "\n",
    "    return n_tasks, changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee56188",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44529c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.functional import relu, avg_pool2d\n",
    "\n",
    "\n",
    "def Xavier(m):\n",
    "    if m.__class__.__name__ == 'Linear':\n",
    "        fan_in, fan_out = m.weight.data.size(1), m.weight.data.size(0)\n",
    "        std = 1.0 * math.sqrt(2.0 / (fan_in + fan_out))\n",
    "        a = math.sqrt(3.0) * std\n",
    "        m.weight.data.uniform_(-a, a)\n",
    "        m.bias.data.fill_(0.0)\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, sizes):\n",
    "        super(MLP, self).__init__()\n",
    "        layers = []\n",
    "\n",
    "        for i in range(0, len(sizes) - 1):\n",
    "            layers.append(nn.Linear(sizes[i], sizes[i + 1]))\n",
    "            if i < (len(sizes) - 2):\n",
    "                layers.append(nn.ReLU())\n",
    "\n",
    "        self.net = nn.Sequential(*layers)\n",
    "        self.net.apply(Xavier)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=False)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(in_planes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion * planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1,\n",
    "                          stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion * planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes, nf):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = nf\n",
    "\n",
    "        self.conv1 = conv3x3(3, nf * 1)\n",
    "        self.bn1 = nn.BatchNorm2d(nf * 1)\n",
    "        self.layer1 = self._make_layer(block, nf * 1, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, nf * 2, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, nf * 4, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, nf * 8, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(nf * 8 * block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        bsz = x.size(0)\n",
    "        out = relu(self.bn1(self.conv1(x.view(bsz, 3, 32, 32))))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def ResNet18(nclasses, nf=20):\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2], nclasses, nf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
